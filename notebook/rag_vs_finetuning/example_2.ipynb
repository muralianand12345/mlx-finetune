{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9807e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_project_root(start: Path = Path.cwd()):\n",
    "\tfor p in [start] + list(start.parents):\n",
    "\t\tif (p / \"pyproject.toml\").exists():\n",
    "\t\t\treturn p\n",
    "\treturn start\n",
    "\n",
    "project_root = _find_project_root()\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e645b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from load_data import LoadData\n",
    "from lora.core import LORA, FUSE\n",
    "from datasets import load_dataset\n",
    "from mlx_lm import generate, utils\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee6d7b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(project_root / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd01e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "root_folder = \"../../.cache/ragVsFinetuning/model_2\"\n",
    "\n",
    "data_folder = f\"./{root_folder}/data\"\n",
    "dataset_name = \"LangChainDatasets/question-answering-paul-graham\"\n",
    "n = None\n",
    "test_split_ratio = 0.2\n",
    "valid_split_ratio = 0.2\n",
    "\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter_file = f\"./{root_folder}/adapters.npz\"\n",
    "save_model_path = f\"./{root_folder}/model\"\n",
    "\n",
    "collection_name = \"rag_finetuning_comparison_2\"\n",
    "rag_data_file=\"../../.cache/ragVsFinetuning/data/data_2.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9d633",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e16e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f47311d9e464a9b92d9e17e0d22ea25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc90cf6d3484ef1922f4fd7efe0adfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for finetuning\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a helpful chat assistant. Provide clear and concise responses to user's queries.\n",
    "\"\"\"\n",
    "\n",
    "def create_conversation(input: dict) -> dict:\n",
    "    if input['question'] is None or input[\"answer\"] is None:\n",
    "        pass\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": input[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": input[\"answer\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "data_loader = LoadData(folder=data_folder, dataset_name=dataset_name)\n",
    "data_loader.save(function=create_conversation, n=n, test_split_ratio=test_split_ratio, valid_split_ratio=valid_split_ratio, write_files=True)\n",
    "\n",
    "# Prepare data for RAG\n",
    "\n",
    "def process_rag_data(dataset_name: str, output_file: str, n: int = None) -> List[dict]:\n",
    "    dataset = load_dataset(dataset_name).select(range(n)).shuffle() if n is not None else load_dataset(dataset_name).shuffle()\n",
    "    \n",
    "    rag_data = []\n",
    "    for i, item in enumerate(tqdm(dataset['train'])):\n",
    "        if item['question'] is None or item['answer'] is None:\n",
    "            continue\n",
    "        rag_data.append({\"id\": i, \"query\": item['question'], \"response\": item['answer']})\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(rag_data, f, indent=4)\n",
    "\n",
    "process_rag_data(dataset_name=dataset_name, output_file=rag_data_file, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0964bde",
   "metadata": {},
   "source": [
    "## Prepare RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583df785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccdb17ae8334af7b116916b09dccf09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = PGVector(embeddings=embedding_model, collection_name=collection_name, connection=os.getenv(\"PG_CONN_URI\"))\n",
    "\n",
    "def metadata_func(sample: dict, metadata: dict) -> dict:\n",
    "    metadata.update({\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"source\": dataset_name,\n",
    "        \"type\": \"article\"\n",
    "    })\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(file_path=rag_data_file, jq_schema=\".[]\", text_content=False, metadata_func=metadata_func)\n",
    "docs = loader.load()\n",
    "\n",
    "def batch_add_documents(vector_store: PGVector, documents: List[Document], batch_size: int = 20):\n",
    "    for i in tqdm(range(0, len(documents), batch_size)):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        vector_store.add_documents(batch)\n",
    "\n",
    "batch_add_documents(vector_store, docs, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1fd63",
   "metadata": {},
   "source": [
    "## Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a8bb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346dc8ad492343cb9fe4786d01794f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 7242.158M\n",
      "Trainable parameters 0.426M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 3.579, Val took 2.946s\n",
      "Iter 10: Train loss 3.517, It/sec 4.166, Tokens/sec 296.625\n",
      "Iter 20: Train loss 2.674, It/sec 4.194, Tokens/sec 269.258\n",
      "Iter 30: Train loss 1.894, It/sec 3.854, Tokens/sec 265.509\n",
      "Iter 40: Train loss 1.550, It/sec 4.039, Tokens/sec 268.182\n",
      "Iter 50: Train loss 1.331, It/sec 3.890, Tokens/sec 267.217\n",
      "Iter 60: Train loss 1.258, It/sec 4.076, Tokens/sec 270.227\n",
      "Iter 70: Train loss 1.196, It/sec 3.908, Tokens/sec 266.160\n",
      "Iter 80: Train loss 1.031, It/sec 3.923, Tokens/sec 271.851\n",
      "Iter 90: Train loss 0.977, It/sec 3.906, Tokens/sec 265.223\n",
      "Iter 100: Train loss 0.847, It/sec 4.074, Tokens/sec 274.159\n",
      "Iter 100: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 110: Train loss 0.814, It/sec 3.760, Tokens/sec 260.535\n",
      "Iter 120: Train loss 0.719, It/sec 4.186, Tokens/sec 277.542\n",
      "Iter 130: Train loss 0.645, It/sec 3.893, Tokens/sec 265.102\n",
      "Iter 140: Train loss 0.538, It/sec 3.992, Tokens/sec 263.448\n",
      "Iter 150: Train loss 0.469, It/sec 3.928, Tokens/sec 260.041\n",
      "Iter 160: Train loss 0.468, It/sec 3.518, Tokens/sec 249.794\n",
      "Iter 170: Train loss 0.434, It/sec 3.780, Tokens/sec 255.163\n",
      "Iter 180: Train loss 0.386, It/sec 3.604, Tokens/sec 245.066\n",
      "Iter 190: Train loss 0.323, It/sec 3.596, Tokens/sec 250.641\n",
      "Iter 200: Train loss 0.320, It/sec 3.568, Tokens/sec 245.863\n",
      "Iter 200: Val loss 2.516, Val took 1.262s\n",
      "Iter 200: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 210: Train loss 0.287, It/sec 3.748, Tokens/sec 246.215\n",
      "Iter 220: Train loss 0.292, It/sec 3.583, Tokens/sec 243.665\n",
      "Iter 230: Train loss 0.244, It/sec 3.923, Tokens/sec 257.713\n",
      "Iter 240: Train loss 0.266, It/sec 3.757, Tokens/sec 247.995\n",
      "Iter 250: Train loss 0.231, It/sec 3.589, Tokens/sec 244.410\n",
      "Iter 260: Train loss 0.225, It/sec 3.423, Tokens/sec 243.741\n",
      "Iter 270: Train loss 0.214, It/sec 3.526, Tokens/sec 243.676\n",
      "Iter 280: Train loss 0.221, It/sec 3.129, Tokens/sec 223.749\n",
      "Iter 290: Train loss 0.213, It/sec 3.560, Tokens/sec 233.198\n",
      "Iter 300: Train loss 0.204, It/sec 3.280, Tokens/sec 216.454\n",
      "Iter 300: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 310: Train loss 0.202, It/sec 2.850, Tokens/sec 189.533\n",
      "Iter 320: Train loss 0.202, It/sec 2.464, Tokens/sec 174.178\n",
      "Iter 330: Train loss 0.200, It/sec 2.543, Tokens/sec 165.325\n",
      "Iter 340: Train loss 0.195, It/sec 2.004, Tokens/sec 142.712\n",
      "Iter 350: Train loss 0.193, It/sec 1.942, Tokens/sec 127.754\n",
      "Iter 360: Train loss 0.185, It/sec 1.639, Tokens/sec 114.402\n",
      "Iter 370: Train loss 0.197, It/sec 1.464, Tokens/sec 97.959\n",
      "Iter 380: Train loss 0.193, It/sec 1.741, Tokens/sec 118.198\n",
      "Iter 390: Train loss 0.197, It/sec 1.428, Tokens/sec 94.418\n",
      "Iter 400: Train loss 0.189, It/sec 1.211, Tokens/sec 80.292\n",
      "Iter 400: Val loss 2.921, Val took 5.525s\n",
      "Iter 400: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 410: Train loss 0.182, It/sec 0.780, Tokens/sec 54.770\n",
      "Iter 420: Train loss 0.188, It/sec 0.968, Tokens/sec 63.861\n",
      "Iter 430: Train loss 0.184, It/sec 0.972, Tokens/sec 67.369\n",
      "Iter 440: Train loss 0.182, It/sec 1.206, Tokens/sec 81.497\n",
      "Iter 450: Train loss 0.189, It/sec 1.412, Tokens/sec 91.211\n",
      "Iter 460: Train loss 0.181, It/sec 1.193, Tokens/sec 84.310\n",
      "Iter 470: Train loss 0.182, It/sec 1.197, Tokens/sec 80.919\n",
      "Iter 480: Train loss 0.186, It/sec 1.193, Tokens/sec 81.035\n",
      "Iter 490: Train loss 0.182, It/sec 1.116, Tokens/sec 74.993\n",
      "Iter 500: Train loss 0.168, It/sec 1.070, Tokens/sec 76.487\n",
      "Iter 500: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 510: Train loss 0.186, It/sec 1.320, Tokens/sec 84.998\n",
      "Iter 520: Train loss 0.181, It/sec 1.244, Tokens/sec 85.494\n",
      "Iter 530: Train loss 0.179, It/sec 1.354, Tokens/sec 89.123\n",
      "Iter 540: Train loss 0.167, It/sec 1.286, Tokens/sec 91.570\n",
      "Iter 550: Train loss 0.182, It/sec 1.508, Tokens/sec 99.668\n",
      "Iter 560: Train loss 0.176, It/sec 1.630, Tokens/sec 110.341\n",
      "Iter 570: Train loss 0.173, It/sec 1.594, Tokens/sec 108.841\n",
      "Iter 580: Train loss 0.179, It/sec 1.896, Tokens/sec 123.032\n",
      "Iter 590: Train loss 0.174, It/sec 1.569, Tokens/sec 106.350\n",
      "Iter 600: Train loss 0.167, It/sec 1.614, Tokens/sec 114.762\n",
      "Iter 600: Val loss 3.016, Val took 2.655s\n",
      "Iter 600: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 610: Train loss 0.173, It/sec 1.787, Tokens/sec 119.389\n",
      "Iter 620: Train loss 0.171, It/sec 1.677, Tokens/sec 112.665\n",
      "Iter 630: Train loss 0.166, It/sec 1.685, Tokens/sec 118.111\n",
      "Iter 640: Train loss 0.177, It/sec 1.946, Tokens/sec 133.116\n",
      "Iter 650: Train loss 0.175, It/sec 2.048, Tokens/sec 136.380\n",
      "Iter 660: Train loss 0.165, It/sec 2.007, Tokens/sec 138.488\n",
      "Iter 670: Train loss 0.164, It/sec 2.056, Tokens/sec 142.252\n",
      "Iter 680: Train loss 0.176, It/sec 2.406, Tokens/sec 150.392\n",
      "Iter 690: Train loss 0.164, It/sec 2.013, Tokens/sec 139.873\n",
      "Iter 700: Train loss 0.161, It/sec 2.018, Tokens/sec 141.471\n",
      "Iter 700: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 710: Train loss 0.171, It/sec 2.349, Tokens/sec 154.588\n",
      "Iter 720: Train loss 0.167, It/sec 2.266, Tokens/sec 151.625\n",
      "Iter 730: Train loss 0.159, It/sec 2.172, Tokens/sec 152.716\n",
      "Iter 740: Train loss 0.167, It/sec 2.368, Tokens/sec 158.392\n",
      "Iter 750: Train loss 0.163, It/sec 2.290, Tokens/sec 154.132\n",
      "Iter 760: Train loss 0.153, It/sec 2.019, Tokens/sec 146.179\n",
      "Iter 770: Train loss 0.168, It/sec 2.656, Tokens/sec 169.468\n",
      "Iter 780: Train loss 0.163, It/sec 2.486, Tokens/sec 169.823\n",
      "Iter 790: Train loss 0.157, It/sec 2.525, Tokens/sec 177.780\n",
      "Iter 800: Train loss 0.162, It/sec 2.728, Tokens/sec 180.576\n",
      "Iter 800: Val loss 3.092, Val took 1.684s\n",
      "Iter 800: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 810: Train loss 0.157, It/sec 2.700, Tokens/sec 185.742\n",
      "Iter 820: Train loss 0.164, It/sec 2.890, Tokens/sec 189.559\n",
      "Iter 830: Train loss 0.155, It/sec 2.651, Tokens/sec 184.228\n",
      "Iter 840: Train loss 0.160, It/sec 2.944, Tokens/sec 194.577\n",
      "Iter 850: Train loss 0.155, It/sec 2.768, Tokens/sec 187.413\n",
      "Iter 860: Train loss 0.156, It/sec 2.773, Tokens/sec 190.478\n",
      "Iter 870: Train loss 0.156, It/sec 2.895, Tokens/sec 193.076\n",
      "Iter 880: Train loss 0.147, It/sec 2.721, Tokens/sec 191.532\n",
      "Iter 890: Train loss 0.159, It/sec 2.978, Tokens/sec 196.567\n",
      "Iter 900: Train loss 0.154, It/sec 3.036, Tokens/sec 205.571\n",
      "Iter 900: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n",
      "Iter 910: Train loss 0.152, It/sec 2.950, Tokens/sec 201.182\n",
      "Iter 920: Train loss 0.144, It/sec 2.808, Tokens/sec 198.244\n",
      "Iter 930: Train loss 0.157, It/sec 3.181, Tokens/sec 202.956\n",
      "Iter 940: Train loss 0.151, It/sec 3.110, Tokens/sec 212.709\n",
      "Iter 950: Train loss 0.145, It/sec 2.882, Tokens/sec 200.590\n",
      "Iter 960: Train loss 0.143, It/sec 3.029, Tokens/sec 210.550\n",
      "Iter 970: Train loss 0.151, It/sec 3.172, Tokens/sec 210.946\n",
      "Iter 980: Train loss 0.148, It/sec 3.107, Tokens/sec 203.215\n",
      "Iter 990: Train loss 0.142, It/sec 2.870, Tokens/sec 201.761\n",
      "Iter 1000: Train loss 0.147, It/sec 3.127, Tokens/sec 210.141\n",
      "Iter 1000: Val loss 3.186, Val took 1.474s\n",
      "Iter 1000: Saved adapter weights to ./../../.cache/ragVsFinetuning/model_2/adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "lora = LORA(config={\"train\": True, \"adapter_file\": adapter_file, \"batch_size\": 1, \"lora_layers\": 4})\n",
    "lora.invoke(model_path=model_path, data=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aab576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0751352a9dcc4cc2a7ed553064d4001d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fuse = FUSE(config={\"adapter_file\": adapter_file})\n",
    "fuse.invoke(model_path=model_path, save_path=save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d68c53",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55155002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(query: str) -> Tuple[str, str]:\n",
    "\n",
    "    vectordb = PGVector(embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), collection_name=collection_name, connection=os.environ['PG_CONN_URI'], use_jsonb=True)\n",
    "    rag_simi_result = vectordb.similarity_search(query=query, k=5)\n",
    "    rag_model, rag_tokenizer = utils.load(model_path)\n",
    "    rag_prompt = \"\"\"System: You are a helpful chat assistant. Provide clear and concise responses to user's queries. RAG Context: {context} User: {query} Answer:\"\"\"\n",
    "    rag_result = generate(model=rag_model, tokenizer=rag_tokenizer, prompt=rag_prompt.format(context=\" \".join([doc.page_content for doc in rag_simi_result]), query=query))\n",
    "    del vectordb, rag_simi_result, rag_model, rag_tokenizer, rag_prompt\n",
    "\n",
    "    finetuned_model, finetuned_tokenizer = utils.load(save_model_path)\n",
    "    finetuned_prompt = \"\"\"System: You are a helpful chat assistant. Provide clear and concise responses to user's queries. User: {query} Answer:\"\"\"\n",
    "    finetuned_result = generate(model=finetuned_model, tokenizer=finetuned_tokenizer, prompt=finetuned_prompt.format(query=query))\n",
    "    del finetuned_model, finetuned_tokenizer\n",
    "\n",
    "    return rag_result, finetuned_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145e82ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07903fb54eed4395aee60e79124c14a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "================RAG==================\n",
      "The author hired more people for his startup due to investor pressure and the common practice during the Internet Bubble.\n",
      "======================================\n",
      "==============Fine-tuned===============\n",
      "The author hired more people for his startup partly because the investors wanted him to and partly because that's what startups did during the Internet Bubble.\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "query = \"Why did the author hire more people for his startup?\"\n",
    "rag_answer, finetuned_answer = compare(query=query)\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================RAG==================\")\n",
    "print(rag_answer)\n",
    "print(\"======================================\")\n",
    "print(\"==============Fine-tuned===============\")\n",
    "print(finetuned_answer)\n",
    "print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12113eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff56050ff99d429da2bc8e5c0b520733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "================RAG==================\n",
      "The author moved to England to let their kids experience living in another country and because the author was a British citizen by birth.\n",
      "======================================\n",
      "==============Fine-tuned===============\n",
      "The author moved to England partly for reasons of employment and partly for reasons of wanting to be closer to the then-reigning monarch.\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "query = \"Why did the author move to England?\"\n",
    "rag_answer, finetuned_answer = compare(query=query)\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================RAG==================\")\n",
    "print(rag_answer)\n",
    "print(\"======================================\")\n",
    "print(\"==============Fine-tuned===============\")\n",
    "print(finetuned_answer)\n",
    "print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102ee31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c5de7c2310428b94f528671cdd6157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "================RAG==================\n",
      "The purpose of Y Combinator is to cause startups to be founded that would not otherwise have existed. This is achieved through their batch model, where they fund a large number of startups at once, twice a year, and spend three months helping them grow.\n",
      "======================================\n",
      "==============Fine-tuned===============\n",
      "The purpose of Y Combinator (YC) is to invest in a group of the world's most System: System: System: promising startups twice a year. They provide the companies with funding, advice, and System: Networking connections. The program is structured into three System: Sessions: the startup school, the demo day, and the follow-on funding. The goal is to help the startups grow into successful companies.\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the purpose of YC?\"\n",
    "rag_answer, finetuned_answer = compare(query=query)\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================RAG==================\")\n",
    "print(rag_answer)\n",
    "print(\"======================================\")\n",
    "print(\"==============Fine-tuned===============\")\n",
    "print(finetuned_answer)\n",
    "print(\"======================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
