{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9807e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_project_root(start: Path = Path.cwd()):\n",
    "\tfor p in [start] + list(start.parents):\n",
    "\t\tif (p / \"pyproject.toml\").exists():\n",
    "\t\t\treturn p\n",
    "\treturn start\n",
    "\n",
    "project_root = _find_project_root()\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e645b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from load_data import LoadData\n",
    "from lora.core import LORA, FUSE\n",
    "from datasets import load_dataset\n",
    "from mlx_lm import generate, utils\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee6d7b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(project_root / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd01e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "root_folder = \"../../.cache/ragVsFinetuning/model_1\"\n",
    "\n",
    "data_folder = f\"./{root_folder}/data\"\n",
    "dataset_name = \"Kaludi/Customer-Support-Responses\"\n",
    "n = None\n",
    "test_split_ratio = 0.2\n",
    "valid_split_ratio = 0.2\n",
    "\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter_file = f\"./{root_folder}/adapters.npz\"\n",
    "save_model_path = f\"./{root_folder}/model\"\n",
    "\n",
    "collection_name = \"rag_finetuning_comparison\"\n",
    "rag_data_file=\"../../.cache/ragVsFinetuning/data/data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9d633",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e16e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for finetuning\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a helpful ticket support agent for company XYZ. Provide clear and concise responses to customer queries.\n",
    "\"\"\"\n",
    "\n",
    "def create_conversation(input: dict) -> dict:\n",
    "    if input['query'] is None or input[\"response\"] is None:\n",
    "        pass\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": input[\"query\"]},\n",
    "            {\"role\": \"assistant\", \"content\": input[\"response\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "data_loader = LoadData(folder=data_folder, dataset_name=dataset_name)\n",
    "data_loader.save(function=create_conversation, n=n, test_split_ratio=test_split_ratio, valid_split_ratio=valid_split_ratio, write_files=True)\n",
    "\n",
    "# Prepare data for RAG\n",
    "\n",
    "def process_rag_data(dataset_name: str, output_file: str, n: int = None) -> List[dict]:\n",
    "    dataset = load_dataset(dataset_name).select(range(n)).shuffle() if n is not None else load_dataset(dataset_name).shuffle()\n",
    "    \n",
    "    rag_data = []\n",
    "    for i, item in enumerate(tqdm(dataset['train'])):\n",
    "        if item['query'] is None or item['response'] is None:\n",
    "            continue\n",
    "        rag_data.append({\"id\": i, \"query\": item['query'], \"response\": item['response']})\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(rag_data, f, indent=4)\n",
    "\n",
    "process_rag_data(dataset_name=dataset_name, output_file=rag_data_file, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0964bde",
   "metadata": {},
   "source": [
    "## Prepare RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583df785",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = PGVector(embeddings=embedding_model, collection_name=collection_name, connection=os.getenv(\"PG_CONN_URI\"))\n",
    "\n",
    "def metadata_func(sample: dict, metadata: dict) -> dict:\n",
    "    metadata.update({\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"source\": dataset_name,\n",
    "        \"type\": \"support_ticket\"\n",
    "    })\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(file_path=rag_data_file, jq_schema=\".[]\", text_content=False, metadata_func=metadata_func)\n",
    "docs = loader.load()\n",
    "\n",
    "def batch_add_documents(vector_store: PGVector, documents: List[Document], batch_size: int = 20):\n",
    "    for i in tqdm(range(0, len(documents), batch_size)):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        vector_store.add_documents(batch)\n",
    "\n",
    "batch_add_documents(vector_store, docs, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1fd63",
   "metadata": {},
   "source": [
    "## Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora = LORA(config={\"train\": True, \"adapter_file\": adapter_file, \"batch_size\": 1, \"lora_layers\": 4})\n",
    "lora.invoke(model_path=model_path, data=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse = FUSE(config={\"adapter_file\": adapter_file})\n",
    "fuse.invoke(model_path=model_path, save_path=save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d68c53",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55155002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(query: str) -> Tuple[str, str]:\n",
    "\n",
    "    vectordb = PGVector(embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), collection_name=collection_name, connection=os.environ['PG_CONN_URI'], use_jsonb=True)\n",
    "    rag_simi_result = vectordb.similarity_search(query=query, k=5)\n",
    "    rag_model, rag_tokenizer = utils.load(model_path)\n",
    "    rag_prompt = \"\"\"System: You are a helpful ticket support agent for company XYZ. Provide clear and concise responses to customer queries. RAG Context: {context} User: {query} Answer:\"\"\"\n",
    "    rag_result = generate(model=rag_model, tokenizer=rag_tokenizer, prompt=rag_prompt.format(context=\" \".join([doc.page_content for doc in rag_simi_result]), query=query))\n",
    "    del vectordb, rag_simi_result, rag_model, rag_tokenizer, rag_prompt\n",
    "\n",
    "    finetuned_model, finetuned_tokenizer = utils.load(save_model_path)\n",
    "    finetuned_prompt = \"\"\"System: You are a helpful ticket support agent for company XYZ. Provide clear and concise responses to customer queries. User: {query} Answer:\"\"\"\n",
    "    finetuned_result = generate(model=finetuned_model, tokenizer=finetuned_tokenizer, prompt=finetuned_prompt.format(query=query))\n",
    "    del finetuned_model, finetuned_tokenizer\n",
    "\n",
    "    return rag_result, finetuned_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145e82ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ab7a30bbcd4d6cb54338f77b64f4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "================RAG==================\n",
      "We apologize for the inconvenience. Could you please provide a clear photo of the damage so we can assess the situation and provide a solution?\n",
      "======================================\n",
      "==============Fine-tuned===============\n",
      "We apologize for the inconvenience. Can you please provide your order number and a description of the damage so we can assist you in resolving the issue?\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "query = \"I received a damaged product.\"\n",
    "rag_answer, finetuned_answer = compare(query=query)\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================RAG==================\")\n",
    "print(rag_answer)\n",
    "print(\"======================================\")\n",
    "print(\"==============Fine-tuned===============\")\n",
    "print(finetuned_answer)\n",
    "print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12113eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd73613baf74465b87a72b7d9ae19b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "================RAG==================\n",
      "Sure, please provide your order number for us to check the current status.\n",
      "======================================\n",
      "==============Fine-tuned===============\n",
      "Certainly. Can you please provide your order number so we can check the current status for you?\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "query = \"I'd like to track my order.\"\n",
    "rag_answer, finetuned_answer = compare(query=query)\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================RAG==================\")\n",
    "print(rag_answer)\n",
    "print(\"======================================\")\n",
    "print(\"==============Fine-tuned===============\")\n",
    "print(finetuned_answer)\n",
    "print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "102ee31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071b6c50b038484f9f052de5dc63b948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "================RAG==================\n",
      "Yes, you can place a bulk order. Please provide the product name or SKU and the quantity you'd like to order for us to check availability and pricing.\n",
      "======================================\n",
      "==============Fine-tuned===============\n",
      "Yes, we do accept bulk orders. Can you provide information on your bulk ordering process?\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "query = \"Can I place a bulk order?\"\n",
    "rag_answer, finetuned_answer = compare(query=query)\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"================RAG==================\")\n",
    "print(rag_answer)\n",
    "print(\"======================================\")\n",
    "print(\"==============Fine-tuned===============\")\n",
    "print(finetuned_answer)\n",
    "print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26368197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
