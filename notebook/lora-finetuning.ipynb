{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace1e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from mlx_lm import generate, utils\n",
    "\n",
    "def _find_project_root(start: Path = Path.cwd()):\n",
    "\tfor p in [start] + list(start.parents):\n",
    "\t\tif (p / \"pyproject.toml\").exists():\n",
    "\t\t\treturn p\n",
    "\treturn start\n",
    "\n",
    "project_root = _find_project_root()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from load_data import LoadData\n",
    "from lora.core import LORA, FUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f44374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "root_folder = \"../.cache/model_1\"\n",
    "\n",
    "data_folder = f\"./{root_folder}/data\"\n",
    "dataset_name = \"b-mc2/sql-create-context\"\n",
    "n = 150\n",
    "test_split_ratio = 0.2\n",
    "valid_split_ratio = 0.2\n",
    "\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter_file = f\"./{root_folder}/adapters.npz\"\n",
    "save_model_path = f\"./{root_folder}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee83b622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0f087f2b6e44f5988798816b0c0b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': './../.cache/model_1/data/train.jsonl',\n",
       " 'test': './../.cache/model_1/data/test.jsonl',\n",
       " 'valid': './../.cache/model_1/data/valid.jsonl'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Train, Test and Validation Data from the Dataset\n",
    "\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "def create_conversation(input: dict) -> dict:\n",
    "    if input['question'] is None or input[\"answer\"] is None or input[\"context\"] is None:\n",
    "        pass\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message.format(schema=input[\"context\"])},\n",
    "            {\"role\": \"user\", \"content\": input[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": input[\"answer\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "data_loader = LoadData(folder=data_folder, dataset_name=dataset_name)\n",
    "data_loader.save(function=create_conversation, n=n, test_split_ratio=test_split_ratio, valid_split_ratio=valid_split_ratio, write_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6b4e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c03f747ac6471095d1f25e53eb71a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 7242.158M\n",
      "Trainable parameters 0.426M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.963, Val took 6.470s\n",
      "Iter 10: Train loss 2.764, It/sec 3.263, Tokens/sec 400.394\n",
      "Iter 20: Train loss 1.939, It/sec 2.886, Tokens/sec 315.745\n",
      "Iter 30: Train loss 1.372, It/sec 2.367, Tokens/sec 287.100\n",
      "Iter 40: Train loss 1.175, It/sec 2.911, Tokens/sec 341.749\n",
      "Iter 50: Train loss 1.000, It/sec 2.846, Tokens/sec 329.331\n",
      "Iter 60: Train loss 1.089, It/sec 2.673, Tokens/sec 351.728\n",
      "Iter 70: Train loss 0.821, It/sec 2.842, Tokens/sec 327.104\n",
      "Iter 80: Train loss 0.917, It/sec 2.981, Tokens/sec 341.962\n",
      "Iter 90: Train loss 0.995, It/sec 3.085, Tokens/sec 368.974\n",
      "Iter 100: Train loss 0.917, It/sec 3.084, Tokens/sec 341.085\n",
      "Iter 100: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 110: Train loss 0.780, It/sec 2.580, Tokens/sec 300.529\n",
      "Iter 120: Train loss 0.799, It/sec 3.061, Tokens/sec 334.239\n",
      "Iter 130: Train loss 0.865, It/sec 2.656, Tokens/sec 321.920\n",
      "Iter 140: Train loss 0.799, It/sec 2.572, Tokens/sec 324.363\n",
      "Iter 150: Train loss 0.793, It/sec 2.940, Tokens/sec 315.750\n",
      "Iter 160: Train loss 0.787, It/sec 2.778, Tokens/sec 331.449\n",
      "Iter 170: Train loss 0.776, It/sec 2.860, Tokens/sec 316.922\n",
      "Iter 180: Train loss 0.870, It/sec 2.564, Tokens/sec 323.262\n",
      "Iter 190: Train loss 0.860, It/sec 2.397, Tokens/sec 312.546\n",
      "Iter 200: Train loss 0.848, It/sec 2.502, Tokens/sec 315.961\n",
      "Iter 200: Val loss 0.841, Val took 7.293s\n",
      "Iter 200: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 210: Train loss 0.725, It/sec 2.781, Tokens/sec 317.363\n",
      "Iter 220: Train loss 0.781, It/sec 2.475, Tokens/sec 326.945\n",
      "Iter 230: Train loss 0.760, It/sec 2.810, Tokens/sec 311.022\n",
      "Iter 240: Train loss 0.671, It/sec 2.705, Tokens/sec 308.372\n",
      "Iter 250: Train loss 0.703, It/sec 2.767, Tokens/sec 325.904\n",
      "Iter 260: Train loss 0.726, It/sec 2.595, Tokens/sec 303.127\n",
      "Iter 270: Train loss 0.766, It/sec 2.777, Tokens/sec 328.508\n",
      "Iter 280: Train loss 0.799, It/sec 2.479, Tokens/sec 315.026\n",
      "Iter 290: Train loss 0.718, It/sec 2.682, Tokens/sec 291.304\n",
      "Iter 300: Train loss 0.682, It/sec 2.909, Tokens/sec 311.604\n",
      "Iter 300: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 310: Train loss 0.704, It/sec 2.659, Tokens/sec 322.553\n",
      "Iter 320: Train loss 0.699, It/sec 2.692, Tokens/sec 301.752\n",
      "Iter 330: Train loss 0.723, It/sec 2.523, Tokens/sec 302.027\n",
      "Iter 340: Train loss 0.690, It/sec 2.664, Tokens/sec 308.526\n",
      "Iter 350: Train loss 0.692, It/sec 2.440, Tokens/sec 314.789\n",
      "Iter 360: Train loss 0.705, It/sec 2.694, Tokens/sec 321.135\n",
      "Iter 370: Train loss 0.679, It/sec 2.617, Tokens/sec 307.554\n",
      "Iter 380: Train loss 0.580, It/sec 2.818, Tokens/sec 318.463\n",
      "Iter 390: Train loss 0.733, It/sec 2.715, Tokens/sec 319.046\n",
      "Iter 400: Train loss 0.659, It/sec 2.592, Tokens/sec 314.680\n",
      "Iter 400: Val loss 0.802, Val took 7.223s\n",
      "Iter 400: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 410: Train loss 0.703, It/sec 2.549, Tokens/sec 301.000\n",
      "Iter 420: Train loss 0.599, It/sec 2.705, Tokens/sec 299.144\n",
      "Iter 430: Train loss 0.688, It/sec 2.601, Tokens/sec 287.953\n",
      "Iter 440: Train loss 0.653, It/sec 2.520, Tokens/sec 309.499\n",
      "Iter 450: Train loss 0.632, It/sec 1.909, Tokens/sec 238.187\n",
      "Iter 460: Train loss 0.560, It/sec 2.009, Tokens/sec 239.243\n",
      "Iter 470: Train loss 0.675, It/sec 2.242, Tokens/sec 260.547\n",
      "Iter 480: Train loss 0.705, It/sec 2.138, Tokens/sec 271.976\n",
      "Iter 490: Train loss 0.613, It/sec 2.400, Tokens/sec 278.640\n",
      "Iter 500: Train loss 0.534, It/sec 2.512, Tokens/sec 270.579\n",
      "Iter 500: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 510: Train loss 0.553, It/sec 2.277, Tokens/sec 246.192\n",
      "Iter 520: Train loss 0.636, It/sec 1.930, Tokens/sec 230.250\n",
      "Iter 530: Train loss 0.576, It/sec 2.143, Tokens/sec 228.208\n",
      "Iter 540: Train loss 0.672, It/sec 1.741, Tokens/sec 222.532\n",
      "Iter 550: Train loss 0.715, It/sec 1.844, Tokens/sec 224.001\n",
      "Iter 560: Train loss 0.587, It/sec 1.574, Tokens/sec 195.199\n",
      "Iter 570: Train loss 0.669, It/sec 1.310, Tokens/sec 174.776\n",
      "Iter 580: Train loss 0.646, It/sec 1.267, Tokens/sec 154.976\n",
      "Iter 590: Train loss 0.551, It/sec 1.450, Tokens/sec 149.188\n",
      "Iter 600: Train loss 0.582, It/sec 1.307, Tokens/sec 152.699\n",
      "Iter 600: Val loss 0.819, Val took 15.846s\n",
      "Iter 600: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 610: Train loss 0.549, It/sec 1.280, Tokens/sec 149.335\n",
      "Iter 620: Train loss 0.706, It/sec 0.859, Tokens/sec 114.280\n",
      "Iter 630: Train loss 0.624, It/sec 1.113, Tokens/sec 140.535\n",
      "Iter 640: Train loss 0.506, It/sec 1.089, Tokens/sec 123.467\n",
      "Iter 650: Train loss 0.531, It/sec 1.082, Tokens/sec 117.107\n",
      "Iter 660: Train loss 0.663, It/sec 0.898, Tokens/sec 110.044\n",
      "Iter 670: Train loss 0.632, It/sec 0.938, Tokens/sec 111.855\n",
      "Iter 680: Train loss 0.532, It/sec 1.020, Tokens/sec 113.808\n",
      "Iter 690: Train loss 0.589, It/sec 0.870, Tokens/sec 112.603\n",
      "Iter 700: Train loss 0.544, It/sec 0.977, Tokens/sec 111.930\n",
      "Iter 700: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 710: Train loss 0.587, It/sec 1.060, Tokens/sec 120.365\n",
      "Iter 720: Train loss 0.610, It/sec 1.034, Tokens/sec 123.394\n",
      "Iter 730: Train loss 0.564, It/sec 1.110, Tokens/sec 124.220\n",
      "Iter 740: Train loss 0.588, It/sec 1.024, Tokens/sec 128.539\n",
      "Iter 750: Train loss 0.609, It/sec 1.169, Tokens/sec 143.209\n",
      "Iter 760: Train loss 0.499, It/sec 1.309, Tokens/sec 148.023\n",
      "Iter 770: Train loss 0.564, It/sec 1.417, Tokens/sec 164.173\n",
      "Iter 780: Train loss 0.492, It/sec 1.478, Tokens/sec 162.277\n",
      "Iter 790: Train loss 0.510, It/sec 1.603, Tokens/sec 178.571\n",
      "Iter 800: Train loss 0.593, It/sec 1.480, Tokens/sec 186.459\n",
      "Iter 800: Val loss 0.811, Val took 11.448s\n",
      "Iter 800: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 810: Train loss 0.539, It/sec 1.620, Tokens/sec 197.934\n",
      "Iter 820: Train loss 0.535, It/sec 1.783, Tokens/sec 201.320\n",
      "Iter 830: Train loss 0.549, It/sec 1.655, Tokens/sec 213.631\n",
      "Iter 840: Train loss 0.560, It/sec 1.629, Tokens/sec 188.179\n",
      "Iter 850: Train loss 0.581, It/sec 1.694, Tokens/sec 203.805\n",
      "Iter 860: Train loss 0.560, It/sec 1.891, Tokens/sec 222.894\n",
      "Iter 870: Train loss 0.528, It/sec 1.844, Tokens/sec 221.869\n",
      "Iter 880: Train loss 0.450, It/sec 1.923, Tokens/sec 212.144\n",
      "Iter 890: Train loss 0.476, It/sec 2.003, Tokens/sec 227.184\n",
      "Iter 900: Train loss 0.618, It/sec 1.716, Tokens/sec 221.226\n",
      "Iter 900: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n",
      "Iter 910: Train loss 0.477, It/sec 1.970, Tokens/sec 220.979\n",
      "Iter 920: Train loss 0.530, It/sec 1.676, Tokens/sec 218.863\n",
      "Iter 930: Train loss 0.435, It/sec 2.012, Tokens/sec 221.096\n",
      "Iter 940: Train loss 0.567, It/sec 1.795, Tokens/sec 203.949\n",
      "Iter 950: Train loss 0.535, It/sec 1.771, Tokens/sec 209.363\n",
      "Iter 960: Train loss 0.585, It/sec 1.756, Tokens/sec 224.819\n",
      "Iter 970: Train loss 0.534, It/sec 1.802, Tokens/sec 232.235\n",
      "Iter 980: Train loss 0.472, It/sec 2.035, Tokens/sec 232.558\n",
      "Iter 990: Train loss 0.472, It/sec 1.745, Tokens/sec 223.249\n",
      "Iter 1000: Train loss 0.475, It/sec 1.839, Tokens/sec 210.434\n",
      "Iter 1000: Val loss 0.834, Val took 9.404s\n",
      "Iter 1000: Saved adapter weights to ./../.cache/model_1/adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tuning with LoRA\n",
    "\n",
    "lora = LORA(config={\"train\": True, \"adapter_file\": adapter_file, \"batch_size\": 1, \"lora_layers\": 4})\n",
    "lora.invoke(model_path=model_path, data=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45121ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ca3557efd34e239cb12fde21877029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fuse the LoRA adapters with the base model and save the fine-tuned model\n",
    "\n",
    "fuse = FUSE(config={\"adapter_file\": adapter_file})\n",
    "fuse.invoke(model_path=model_path, save_path=save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97cf299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT season_number FROM table_13505192_3 WHERE season_number = \"24\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model and generate a response\n",
    "\n",
    "model, tokenizer = utils.load(save_model_path)\n",
    "generate(model=model, tokenizer=tokenizer, prompt=\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_13505192_3 (series_number INTEGER, season_number VARCHAR)\\nUser:What is the series number for season episode 24?\\nAssistant:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce29d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
